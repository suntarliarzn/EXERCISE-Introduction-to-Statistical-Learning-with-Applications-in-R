---
title: "10.9.6.1 Recurrent Neural Networks with text"
output: github_document
---

# 1.data preparation
```{R}
library(keras)
library(tensorflow)
max_feature <- 10000
imdb <- dataset_imdb(num_words=max_feature)
c(c(x_train,y_train),c(x_test,y_test)) %<-% imdb
```

# 2.decode word
```{R}
word_index <- dataset_imdb_word_index()
decode_review <- function(text, word_index){
  word <- names(word_index)
  idx <- unlist(word_index, use.names=FALSE)
  word <- c("<PAD>", "<START>", "<UNK>", "<UNUSED>", word)
  idx <- c(0:3, idx+3)
  words <- word[match(text,idx,2)]
  paste(words, collapse=" ")
}
decode_review(x_train[[1]][1:12], word_index)
```

# 3.one-hot encoding
```{R}
library(Matrix)
one_hot <- function(sequences, dimension){
  seqlen <- sapply(sequences,length)
  n <- length(seqlen)
  rowind <- rep(1:n, seqlen)
  colind <- unlist(sequences)
  sparseMatrix(i=rowind, j=colind, dims=c(n,dimension))
}
x_train_1h <- one_hot(x_train,10000)
x_test_1h <- one_hot(x_test,10000)
dim(x_train_1h)
head(x_train_1h)
nnzero(x_train_1h)/(25000*10000)
```

# 4.data with same length
```{R}
wl <- sapply(x_train,length)
median(wl)
sum(wl<= 500)/length(wl)
maxlen <- 500
# decode_review(x_train[[958]],word_index)
x_train <- pad_sequences(x_train, maxlen=maxlen)
x_test <- pad_sequences(x_test, maxlen=maxlen)
# decode_review(x_train[958,],word_index)
```

# 5.RNN with LSTM
```{r}
model <- keras_model_sequential()
model %>% layer_embedding(input_dim=10000, output_dim = 32) %>% 
  layer_lstm(units=32) %>% 
  layer_dense(units = 1, activation="sigmoid")
model %>% compile(optimizer="rmsprop", loss="binary_crossentropy", metrics=c("acc"))
model %>% fit(x_train , y_train, epochs=10, batch_size=128, validation_data=list(x_test,y_test))
pred <- predict(model,x_test)>0.5
mean(abs(y_test==as.numeric(pred)))
# 0.85872
```
