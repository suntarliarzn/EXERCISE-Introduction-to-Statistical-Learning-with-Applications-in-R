devtools::install_github("rstudio/tensorflow")
devtools::install_github("rstudio/keras")
install_keras()
library(keras)
install_keras()
y
library(tensorflow)
install_tnesorflow()
install_tensorflow()
mnist.load_data()
library(keras)
dataset_mnist()
head(dataset_mnist())
string(dataset_mnist())
dim(dataset_mnist())
summary(dataset_mnist())
mnist <- dataset_mnist()
?dataset_mnist
dim(mnist$train)
summary(mnist$train)
head(mnist$train)
mnist$train$x[1]
mnist$train$x[1,]
mnist$train$x
mnist$train$x[,1]
mnist$train$x[,10]
mnist$train$x[[[,]]]
mnist$train$x[[,]]
mnist$train$x[,,1]
mnist$train$x[,,2]
mnist$train$x[,,2][,1]
install.packages("read.table")
# install.packages("read.table")
library(read.table)
# install.packages("read.table")
# library(read.table)
iris <- read.csv(url("http://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data"), header = FALSE)
head(iris)
dim(iris)
str(iris)
?iris
library(keras)
# 1. Rtools
# https://cran.r-project.org/bin/windows/Rtools/rtools40.html
# 2. Update R
# https://www.linkedin.com/pulse/3-methods-update-r-rstudio-windows-mac-woratana-ngarmtrakulchol/
# 3. Installation tenserflow and keas
# install.packages("Rcpp")
# install.packages("devtools")
# library(devtools)
# devtools::install_github("rstudio/reticulate", force=TRUE)
# devtools::install_github("r-lib/processx")
# library(processx)
# devtools::install_github("rstudio/tensorflow")
# devtools::install_github("rstudio/keras")
# library(keras)
# install_keras()
# library(tensorflow)
# install_tensorflow()
setwd("~/")
# 1. Rtools
# https://cran.r-project.org/bin/windows/Rtools/rtools40.html
# 2. Update R
# https://ww?.linkedin.com/pulse/3-methods-update-r-rstudio-windows-mac-woratana-ngarmtrakulchol/
# 3. Installation tenserflow and keas
# install.packages("Rcpp")
# install.packages("devtools")
# library(devtools)
# devtools::install_github("rstudio/reticulate", force=?RUE)
# devtools::install_github("r-lib/processx")
# library(processx)
# devtools::install_github("rstudio/tensorflow")
# devtools::install_github("rstudio/keras")
library(keras)
library(tensorflow)
iris <- read.csv(url("http://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data"), header = FALSE)
head(iris)
dim(iris)
str(iris)
names(iris) <- c("Sepal.Length", "Sepal.Width", "Petal.Length", "Petal.Width", "Species")
iris[,5] <- as.factor(iris[,5])
plot(iris$Petal.Length, iris$Petal.Width, pch=21, col=c("red","green3","blue")[unclass(iris$Species)], xlab="Petal Length", ylab="Petal Width")
cor(iris$Petal.Length, iris$Petal.Width)
m <- cor(iris[,1:4])
library(corrplot)
corrplot(m)
iris_mat <-as.matrix(iris[,1:4])
iris_nor <- normalize(iris_mat)
iris_mat <-as.matrix(iris[,1:4])
iris_nor <- normalize(iris_mat)
iris_mat <-as.matrix(iris[,1:4])
iris_nor <- normalize(iris_mat)
iris_mat <-as.matrix(iris[,1:4])
iris_nor <- normalize(iris_mat)
set.seed(1)
ind <- sample(2, nrow(iris), replace=TRUE, prob=c(0.67, 0.33))
iris_train <- iris_nor[ind==1,]
iris_test <- iris_nor[ind==2,]
iris_train_y <- as.numeric(iris[ind==1,5])-1
iris_test_y <- as.numeric(iris[ind==2,5])-1
# One hot encode training target values
iris_train_Labels <- to_categorical(iris_train_y)
# One hot encode test target values
iris_test_Labels <- to_categorical(iris_test_y)
# Print out the iris.testLabels to double check the result
print(iris_test_Labels)
# 定义要使用的模型
model <- keras_model_sequential()
# 模型定义输入数据(input_shape)为4列,layer_dense为2层，units为每层node数量，activation为每层使用的转换函数。
model %>% layer_dense(units = 4,activation="relu", input_shape = c(4)) %>% layer_dense(units=3, activation= "softmax")
# Print a summary of a model
summary(model)
# Get model configuration
get_config(model)
# Get layer configuration
get_layer(model, index = 1)
# List the model's layers
model$layers
# List the input tensors
model$inputs
# List the output tensors
model$outputs
# Some of the most popular optimization algorithms used are the Stochastic Gradient Descent (SGD), ADAM and RMSprop. Depending on whichever algorithm you choose, you’ll need to tune certain parameters, such as learning rate or momentum. The choice for a loss function depends on the task that you have at hand: for example, for a regression problem, you’ll usually use the Mean Squared Error (MSE).
#
# As you see in this example, you used categorical_crossentropy loss function for the multi-class classification problem of determining whether an iris is of type versicolor, virginica or setosa. However, note that if you would have had a binary-class classification problem, you should have made use of the binary_crossentropy loss function.
model %>% compile(loss="categorical_crossentropy", optimizer="SGD", metrics="accuracy")
# An epoch is a single pass through the entire training set, followed by testing of the verification set. The batch size that you specify in the code above defines the number of samples that going to be propagated through the network.
model %>% fit(iris_train,iris_train_Labels, epochs = 100, batch_size = 5, validation_split = 0.2)
classes <- model %>% predict(iris_test) %>% k_argmax()
table(iris_test_y, as.numeric(classes))
# 定义要使用的模型
model <- keras_model_sequential()
# 模型定义输入数据(input_shape)为4列,layer_dense为2层，units为每层node数量，activation为每层使用的转换函数。
model %>% layer_dense(units = 24,activation="relu", input_shape = c(4)) %>% layer_dense(units=3, activation= "softmax")
# Print a summary of a model
summary(model)
# Get model configuration
get_config(model)
# Get layer configuration
get_layer(model, index = 1)
# List the model's layers
model$layers
# List the input tensors
model$inputs
# List the output tensors
model$outputs
# Some of the most popular optimization algorithms used are the Stochastic Gradient Descent (SGD), ADAM and RMSprop. Depending on whichever algorithm you choose, you’ll need to tune certain parameters, such as learning rate or momentum. The choice for a loss function depends on the task that you have at hand: for example, for a regression problem, you’ll usually use the Mean Squared Error (MSE).
#
# As you see in this example, you used categorical_crossentropy loss function for the multi-class classification problem of determining whether an iris is of type versicolor, virginica or setosa. However, note that if you would have had a binary-class classification problem, you should have made use of the binary_crossentropy loss function.
model %>% compile(loss="categorical_crossentropy", optimizer="SGD", metrics="accuracy")
# An epoch is a single pass through the entire training set, followed by testing of the verification set. The batch size that you specify in the code above defines the number of samples that going to be propagated through the network.
model %>% fit(iris_train,iris_train_Labels, epochs = 100, batch_size = 5, validation_split = 0.2)
classes <- model %>% predict(iris_test) %>% k_argmax()
table(iris_test_y, as.numeric(classes))
# Initialize the sequential model
model <- keras_model_sequential()
# Add layers to model
model %>%
layer_dense(units = 8, activation = 'relu', input_shape = c(4)) %>%
layer_dense(units = 5, activation = 'relu') %>%
layer_dense(units = 3, activation = 'softmax')
# Compile the model
model %>% compile(
loss = 'categorical_crossentropy',
optimizer = 'SGD',
metrics = 'accuracy'
)
# Fit the model to the data
model %>% fit(iris_train,iris_train_Labels, epochs = 200, batch_size = 5, validation_split = 0.2)
# Evaluate the model
score <- model %>% evaluate(iris_test, iris_test_Labels, batch_size = 128)
# Print the score
print(score)
library(keras)
mnist <- dataset_mnist()
library(keras)
mnist <- dataset_mnist()
x_train <- mnist$train$x
y_train <- mnist$train$y
x_test <- mnist$test$x
y_test <- mnist$test$y
head(x_train)
str(x_train)
?array_reshape
28*28
x_train <- array_reshape(x_train, c(nrow(x_train)))
x_train <- array_reshape(x_train, c(nrow(x_train)), 28*28)
x_train <- array_reshape(x_train, c(nrow(x_train)), 784)
x_train <- array_reshape(x_train, c(nrow(x_train), 784)
x_train <- array_reshape(x_train, c(nrow(x_train), 784)
x_train <- array_reshape(x_train, c(nrow(x_train), 784))
x_train_mat <- array_reshape(x_train, c(nrow(x_train), 784))
View(x_train_mat)
x_train_mat <- array_reshape(x_train, c(nrow(x_train), 784))
x_test_mat <- array_reshape(x_test,c(nrow(x_test),784))
y_train_cat <-to_categorical(y_train)
y_test_cat <- to_categorical(y_test)
head(y_train_cat)
head(y-train)
head(y_train)
x_train_mat <- array_reshape(x_train, c(nrow(x_train), 784))
x_test_mat <- array_reshape(x_test,c(nrow(x_test),784))
y_train_cat <-to_categorical(y_train)
y_test_cat <- to_categorical(y_test)
x_train_mat <- x_train_mat/255
x_test_mat <- x_test_mat/255
head(x_train_mat)
?drop
model <- keras_model_sequential()
model %>% layer_dense(units=256, activation="relu", input_shape=c(784)) %>% layer_dropout(rate=0.4) %>% layer_dense(units=128, activation="relu") %>% layer_dropout(rate=0.3) %>% layer_dense(units=10, activation="softmax")
summary(model)
model %>% compile(loss="categorical_crossentropy", optimizer=optimizer_rmsprop(), metrics=c("accuracy"))
fit(x_train,y_train, epochs=50, batch_size=128, validation_split=0.2)
model <- keras_model_sequential()
model %>% layer_dense(units=256, activation="relu", input_shape=c(784)) %>% layer_dropout(rate=0.4) %>% layer_dense(units=128, activation="relu") %>% layer_dropout(rate=0.3) %>% layer_dense(units=10, activation="softmax")
summary(model)
model %>% compile(loss="categorical_crossentropy", optimizer=optimizer_rmsprop(), metrics=c("accuracy"))
fit(x_train_mat,y_train_cat, epochs=50, batch_size=128, validation_split=0.2)
model <- keras_model_sequential()
model %>% layer_dense(units=256, activation="relu", input_shape=c(784)) %>% layer_dropout(rate=0.4) %>% layer_dense(units=128, activation="relu") %>% layer_dropout(rate=0.3) %>% layer_dense(units=10, activation="softmax")
summary(model)
model %>% compile(loss="categorical_crossentropy", optimizer=optimizer_rmsprop(), metrics=c("accuracy"))
mdoel %>% fit(x_train_mat,y_train_cat, epochs=50, batch_size=128, validation_split=0.2)
model <- keras_model_sequential()
model %>% layer_dense(units=256, activation="relu", input_shape=c(784)) %>% layer_dropout(rate=0.4) %>% layer_dense(units=128, activation="relu") %>% layer_dropout(rate=0.3) %>% layer_dense(units=10, activation="softmax")
summary(model)
model %>% compile(loss="categorical_crossentropy", optimizer=optimizer_rmsprop(), metrics=c("accuracy"))
model %>% fit(x_train_mat,y_train_cat, epochs=50, batch_size=128, validation_split=0.2)
accuracy <- function(pred,truth){
mean(drop(pred)==drop(truth))
}
model %>% predict_classes(x_test) %>% accuracy(y_test)
predict_classes(x_test)
model %>% predict(x_test_mat) %>% k_argmax()
y_predict <- model %>% predict(x_test_mat) %>% k_argmax()
head(y_predict)
drop(y_predict)
y_predict
as.numeric(y_predict)
as.numeric(drop(y_predict))
accuracy(y_predict,y_test)
y_test
mean(y_predict==y_test)
mean(drop(y_predict)==drop(y_test))
mean(as.numeric(y_predict)==as.numeric(y_test))
model <- keras_model_sequential()
model %>% layer_dense(units=256, activation="relu", input_shape=c(784)) %>% layer_dropout(rate=0.4) %>% layer_dense(units=128, activation="relu") %>% layer_dropout(rate=0.3) %>% layer_dense(units=10, activation="softmax")
summary(model)
model %>% compile(loss="categorical_crossentropy", optimizer=optimizer_rmsprop(), metrics=c("accuracy"))
model %>% fit(x_train_mat,y_train_cat, epochs=10, batch_size=32, validation_split=0.2)
y_predict <- model %>% predict(x_test_mat) %>% k_argmax()
mean(drop(y_predict)==drop(y_test))
mean(as.numeric(y_predict)==as.numeric(y_test))
modellr <- keras_model_sequential()
modellr %>% layer_dense(units=10, activation="softmax", input_shape=c(784))
summary(model)
modellr %>% compile(loss="categorical_crossentropy", optimizer=optimizer_rmsprop(), metrics=c("accuracy"))
modellr %>% fit(x_train_mat,y_train_cat, epochs=10, batch_size=32, validation_split=0.2)
y_predict <- modellr %>% predict(x_test_mat) %>% k_argmax()
mean(as.numeric(y_predict)==as.numeric(y_test))
library(keras)
cifar100 <- dataset_cifar100()
dataset_cifar100()
dataset_cifar10()
import ssl
ssl._create_default_https_context = ssl._create_unverified_context
verify-ssl-certs=0
read.table(E:/little-trowel/02_TC/04 Tools/4.2 R/10. deep learning/cifar-100-python/cifar-100-python/train)
read.table(E:\little-trowel\02_TC\04 Tools\4.2 R\10. deep learning\cifar-100-python\cifar-100-python\train)
read.table("E:/little-trowel/02_TC/04 Tools/4.2 R/10. deep learning/cifar-100-python/cifar-100-python/train")
library(keras)
library(reticulate)
cifar100 <- dataset_cifar100()
library(keras)
library(reticulate)
library(keras)
library(reticulate)
library(keras)
library(tensorflow)
library(reticulate)
library(keras)
library(tensorflow)
library(reticulate)
install_keras()
install_tensorflow()
dataset_cifar100()
cifar100 <- py$cifar100
use_python("/use/bin/python")
cifar100 <- py$cifar100
use_python("/user/bin/python")
use_python("/usr/bin/python")
library(keras)
library(tensorflow)
library(reticulate)
# install_keras()
# install_tensorflow()
cifar100 <- py$cifar100
cifar100 <- py$datasets.cifar100.load_data()
cifar100 <- py$my_pthon_array
library(keras)
library(tensorflow)
library(reticulate)
# install_keras()
# install_tensorflow()
cifar100 <- py$my_pthon_array
cifar100 <- py$my_pthon_array
library(keras)
library(tensorflow)
library(reticulate)
use_python()
library(keras)
library(tensorflow)
library(reticulate)
use_python("C:/Users/suntar/AppData/Roaming/Microsoft/Windows/Start Menu/Programs/Python 3.7")
library(keras)
library(tensorflow)
library(reticulate)
use_python("C:/Users/suntar/AppData/Local/r-miniconda/python.exe")
use_python("C:/Users/suntar/AppData/Local/r-miniconda/pkgs/python.-3.9.7-h6244533_1/python.exe")
use_python("C:/Users/suntar/AppData/Local/r-miniconda/pkgs/python-3.9.7-h6244533_1/python.exe")
use_python("C:/Users/suntar/AppData/Local/r-miniconda/pkgs/python-3.9.7-h6244533_1/python")
use_python()
my_python_array
py$r
py$r$my_python_array
py$my_python_array
cifar100 <- py$dataset
cifar100 <- py$dataset=unpickle(file)
cifar100 <- py$unpickle(file)
library(keras)
library(tensorflow)
library(reticulate)
use_python_version(3.9.7)
library(keras)
library(tensorflow)
library(reticulate)
use_python_version("3.9.7")
?reticulate
py_exe()
py_run_string(ssl._create_default_https_context = ssl._create_unverified_context)
py_run_string("ssl._create_default_https_context = ssl._create_unverified_context")
py_run_string("import ssl")
py_run_string("ssl._create_default_https_context = ssl._create_unverified_context")
py_run_string("import ssl")
py_run_string("ssl._create_default_https_context = ssl._create_unverified_context")
cifiar <- dataset_cifar100()
dim(cifiar)
head(cifiar)
names(cifar100)
names(cifiar)
x_train <- cifiar$train$x
g_train <- cifiar$train$y
x_test <- cifiar$test$x
g_test <- cifiar$test$y
dim(x_train)
head(x_test)
head(g_test)
summary(x_train)
str(x_train)
names(cifiar)
x_train <- cifiar$train$x
g_train <- cifiar$train$y
x_test <- cifiar$test$x
g_test <- cifiar$test$y
dim(x_train)
head(x_test)
head(g_test)
x_train <- x_train/255
x_test <- x_test/255
y_train <- to_categorical(g_train,100)
install.packages("jpeg")
library(jpeg)
par(mar = c(0, 0, 0, 0), mfrow = c(5, 5))
index <- sample(seq(50000) , 25)
for (i in index)
plot(as.raster(x_train[i,,, ]))
?layer_conv_1d
layer_conv_2d()
?layer_conv_2d()
model %>% keras_model_sequential()
library(keras)
library(tensorflow)
library(reticulate)
# install_keras()
# install_tensorflow()
# 传递命令给python， 借助下载安全验证
py_run_string("import ssl")
py_run_string("ssl._create_default_https_context = ssl._create_unverified_context")
cifiar <- dataset_cifar100()
model %>% keras_model_sequential()
model <-  keras_model_sequential()
model %>% layer_conv_2d(filters = 32, kernel_size=c(3,3), padding="same", activation="relu", input_shape=c(32,32,3)) %>% layer_max_pooling_2d(pool_size=c(2,2)) %>%
layer_conv_2d(filters=64, kernel_size=c(3,3), padding="same", activation="relu") %>% layer_max_pooling_2d(pool_size = c(2,2)) %>%
layer_conv_2d(filters=128, kernel_size=c(3,3), padding="same", activation="relu") %>% layer_max_pooling_2d(pool_size = c(2,2)) %>%
layer_conv_2d(filters=256, kernel_size=c(3,3), padding="same", activation="relu") %>% layer_max_pooling_2d(pool_size = c(2,2)) %>%
layer_flatten() %>%
layer_dropout(rate=0.5) %>%
layer_dense(units = 512, activation="relu") %>%
layer_dense(unites=100,activation = "softmax")
model <-  keras_model_sequential()
model %>% layer_conv_2d(filters = 32, kernel_size=c(3,3), padding="same", activation="relu", input_shape=c(32,32,3)) %>% layer_max_pooling_2d(pool_size=c(2,2)) %>%
layer_conv_2d(filters=64, kernel_size=c(3,3), padding="same", activation="relu") %>% layer_max_pooling_2d(pool_size = c(2,2)) %>%
layer_conv_2d(filters=128, kernel_size=c(3,3), padding="same", activation="relu") %>% layer_max_pooling_2d(pool_size = c(2,2)) %>%
layer_conv_2d(filters=256, kernel_size=c(3,3), padding="same", activation="relu") %>% layer_max_pooling_2d(pool_size = c(2,2)) %>%
layer_flatten() %>%
layer_dropout(rate=0.5) %>%
layer_dense(units = 512, activation="relu") %>%
layer_dense(units=100,activation = "softmax")
model %>% compile(loss="categorical_crossentropy", optimizer=optimizer_rmsprop(), metrics=c("accuracy"))
model %>% fit(x_train,y_train, epochs=30, batch_size=128, validation_split=0.2)
y_predict <- model %>% predict(x_test) %>% k_argmax()
mean(as.numeric(y_predict)==as.numeric(y_test))
mean(as.numeric(y_predict)==as.numeric(g_test))
library(keras)
library(tensorflow)
library(reticulate)
# install_keras()
# install_tensorflow()
# 传递命令给python， 借助下载安全验证
py_run_string("import ssl")
py_run_string("ssl._create_default_https_context = ssl._create_unverified_context")
cifiar <- dataset_cifar100()
names(cifiar)
x_train <- cifiar$train$x
g_train <- cifiar$train$y
x_test <- cifiar$test$x
g_test <- cifiar$test$y
dim(x_train)
head(x_test)
head(g_test)
x_train <- x_train/255
x_test <- x_test/255
y_train <- to_categorical(g_train,100)
library(jpeg)
par(mar = c(0, 0, 0, 0), mfrow = c(5, 5))
index <- sample(seq(50000) , 25)
for (i in index)
plot(as.raster(x_train[i,,, ]))
model <-  keras_model_sequential()
model %>% layer_conv_2d(filters = 32, kernel_size=c(3,3), padding="same", activation="relu", input_shape=c(32,32,3)) %>% layer_max_pooling_2d(pool_size=c(2,2)) %>%
layer_conv_2d(filters=64, kernel_size=c(3,3), padding="same", activation="relu") %>% layer_max_pooling_2d(pool_size = c(2,2)) %>%
layer_conv_2d(filters=128, kernel_size=c(3,3), padding="same", activation="relu") %>% layer_max_pooling_2d(pool_size = c(2,2)) %>%
layer_conv_2d(filters=256, kernel_size=c(3,3), padding="same", activation="relu") %>% layer_max_pooling_2d(pool_size = c(2,2)) %>%
layer_flatten() %>%
layer_dropout(rate=0.5) %>%
layer_dense(units = 512, activation="relu") %>%
layer_dense(units=100,activation = "softmax")
model %>% compile(loss="categorical_crossentropy", optimizer=optimizer_rmsprop(), metrics=c("accuracy"))
model %>% fit(x_train,y_train, epochs=10, batch_size=64, validation_split=0.2)
y_predict <- model %>% predict(x_test) %>% k_argmax()
mean(as.numeric(y_predict)==as.numeric(g_test))
setwd("~/")
setwd("E:/little-trowel/02_TC/04 Tools/4.2 R/10. deep learning")
library(keras)
library(tensorflow)
img_dir <- "book_images"
image_names <- list.files(img_dir)
num_images <- length(image_names)
image_names
num_images
x <- array(dim=c(num_images,224,224,3))
head(x)
?paste
paste("foo", {}, "bar", collapse="|")
img_dir
?image_load
paste(img_dir, image_names[1], sep="/")
paste(img_dir, image_names[2], sep="/")
image_to_array(image_names[1])
x <- array(dim=c(num_images,224,224,3))
for(i in 1:num_images){
img_path <- paste(img_dir, image_names[i], sep="/")
img <- image_load(img_path, target_size = 224,224)
x[i,,,] <- image_to_array(img)
}
x <- array(dim=c(num_images,224,224,3))
for(i in 1:num_images){
img_path <- paste(img_dir, image_names[i], sep="/")
img <- image_load(img_path, target_size = c(224,224))
x[i,,,] <- image_to_array(img)
}
x <- imagenet_preprocess_input(x)
head(x[1,,,])
str(x[1,,,])
?image_to_array
image_array_save(img[1],"train")
image_array_save(x[1],"train")
image_array_save(x[1,,,],"train")
image_array_save(x[1,,,],"train.jpg")
model <- application_resnet50(weights="imagenet")
summary(model)
pred <- model %>% predict(x) %>% imagenet_decode_predictions(top=3)
names(pred) <- image_names
print(pred)
library(keras)
library(tensorflow)
img_dir <- "book_images"
image_names <- list.files(img_dir)
num_images <- length(image_names)
x <- array(dim=c(num_images,224,224,3))
for(i in 1:num_images){
img_path <- paste(img_dir, image_names[i], sep="/")
img <- image_load(img_path, target_size = c(224,224))
x[i,,,] <- image_to_array(img)
}
# image_array_save(x[1,,,],"train.jpg")
x <- imagenet_preprocess_input(x)
model <- application_resnet50(weights="imagenet")
summary(model)
pred <- model %>% predict(x) %>% imagenet_decode_predictions(top=3)
names(pred) <- image_names
print(pred)
