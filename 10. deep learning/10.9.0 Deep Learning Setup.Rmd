---
title: "10.9.0 Deep Learning"
output: github_document
---

# https://www.datacamp.com/community/tutorials/keras-r-deep-learning

# 1.Installation
```{R}
# 1. Rtools
# https://cran.r-project.org/bin/windows/Rtools/rtools40.html
# 2. Update R
# https://ww?.linkedin.com/pulse/3-methods-update-r-rstudio-windows-mac-woratana-ngarmtrakulchol/
# 3. Installation tenserflow and keras
# install.packages("Rcpp")
# install.packages("devtools")
# library(devtools)
# devtools::install_github("rstudio/reticulate", force=TRUE)
# devtools::install_github("r-lib/processx")
# library(processx)
# devtools::install_github("rstudio/tensorflow")
# devtools::install_github("rstudio/keras")
library(keras)
# install_keras()
library(tensorflow)
# install_tensorflow()
```

# 2.load Dataset
```{R}
iris <- read.csv(url("http://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data"), header = FALSE) 
head(iris)
dim(iris)
str(iris)
```

# 3. Name the dataframe
```{R}
names(iris) <- c("Sepal.Length", "Sepal.Width", "Petal.Length", "Petal.Width", "Species")
iris[,5] <- as.factor(iris[,5])
plot(iris$Petal.Length, iris$Petal.Width, pch=21, col=c("red","green3","blue")[unclass(iris$Species)], xlab="Petal Length", ylab="Petal Width")
cor(iris$Petal.Length, iris$Petal.Width)
m <- cor(iris[,1:4])
library(corrplot)
corrplot(m)
```

# 4.data process
```{R}
iris_mat <-as.matrix(iris[,1:4]) 
iris_nor <- normalize(iris_mat)
```

# 5.data_separation
```{R}
set.seed(119)
ind <- sample(2, nrow(iris), replace=TRUE, prob=c(0.67, 0.33))
iris_train <- iris_nor[ind==1,]
iris_test <- iris_nor[ind==2,]
iris_train_y <- as.numeric(iris[ind==1,5])-1
iris_test_y <- as.numeric(iris[ind==2,5])-1
```

# 6. One-Hot Encoding
```{R}
# One hot encode training target values
iris_train_Labels <- to_categorical(iris_train_y)

# One hot encode test target values
iris_test_Labels <- to_categorical(iris_test_y)

# Print out the iris.testLabels to double check the result
print(iris_test_Labels)
```

# 7. neural network with two layers
```{R}
# 定义要使用的模型
model <- keras_model_sequential()
# 模型定义输入数据(input_shape)为4列,layer_dense为2层，units为每层node数量，activation为每层使用的转换函数。
model %>% layer_dense(units = 8,activation="relu", input_shape = c(4)) %>% layer_dense(units=3, activation= "softmax")

# Print a summary of a model
summary(model)

# Get model configuration
get_config(model)

# Get layer configuration
get_layer(model, index = 1)

# List the model's layers
model$layers

# List the input tensors
model$inputs

# List the output tensors
model$outputs
```

# 8. compile the model
```{R}
# Some of the most popular optimization algorithms used are the Stochastic Gradient Descent (SGD), ADAM and RMSprop. Depending on whichever algorithm you choose, you’ll need to tune certain parameters, such as learning rate or momentum. The choice for a loss function depends on the task that you have at hand: for example, for a regression problem, you’ll usually use the Mean Squared Error (MSE).
# 
# As you see in this example, you used categorical_crossentropy loss function for the multi-class classification problem of determining whether an iris is of type versicolor, virginica or setosa. However, note that if you would have had a binary-class classification problem, you should have made use of the binary_crossentropy loss function.
model %>% compile(loss="categorical_crossentropy", optimizer="SGD", metrics="accuracy")

# An epoch is a single pass through the entire training set, followed by testing of the verification set. The batch size that you specify in the code above defines the number of samples that going to be propagated through the network.
model %>% fit(iris_train,iris_train_Labels, epochs = 500, batch_size = 5, validation_split = 0.2)
```

# 9.visualize the model training history
```{R}
# Store the fitting history in `history` 
history <- model %>% fit( iris_train,iris_train_Labels, epochs = 500, batch_size = 5, validation_split = 0.2)
# Plot the model loss of the training data
plot(history$metrics$loss, main="Model Loss", xlab = "epoch", ylab="loss", col="blue", type="l", ylim =c(0,1))

# Plot the model loss of the test data
lines(history$metrics$val_loss, col="green")

# Add legend
legend("topright", c("train","test"), col=c("blue", "green"), lty=c(1,1))

# Plot the accuracy of the training data 
plot(history$metrics$acc, main="Model Accuracy", xlab = "epoch", ylab="accuracy", col="orange", type="l", ylim =c(0.5,1))

# Plot the accuracy of the validation data
lines(history$metrics$val_acc, col="red")

# Add Legend
legend("bottomright", c("train","test"), col=c("orange", "red"), lty=c(1,1))
```

# 10.predict new data
```{R}
classes <- model %>% predict(iris_test) %>% k_argmax()
table(iris_test_y, as.numeric(classes))
```

# 11.evaluating model
```{R}
score <- model %>% evaluate(iris_test, iris_test_Labels, batch_size=128)
print(score)
```

# 12. fine-tuning the model
## 12.1 add layers
```{R}
# Initialize the sequential model
model1 <- keras_model_sequential() 

# Add layers to model
model1 %>% 
    layer_dense(units = 8, activation = 'relu', input_shape = c(4)) %>% 
    layer_dense(units = 5, activation = 'relu') %>% 
    layer_dense(units = 3, activation = 'softmax')

# Compile the model
model1 %>% compile(
     loss = 'categorical_crossentropy',
     optimizer = 'SGD',
     metrics = 'accuracy'
 )

# Fit the model to the data
model1 %>% fit(iris_train,iris_train_Labels, epochs = 500, batch_size = 5, validation_split = 0.2)

# Evaluate the model
score <- model1 %>% evaluate(iris_test, iris_test_Labels, batch_size = 128)

# Print the score
print(score)
```

## 12.2 add hidden units
```{R}
# Initialize the sequential model
model2 <- keras_model_sequential() 

# Add layers to model
model2 %>% 
    layer_dense(units = 24, activation = 'relu', input_shape = c(4)) %>% 
    layer_dense(units = 3, activation = 'softmax')

# Compile the model
model2 %>% compile(
     loss = 'categorical_crossentropy',
     optimizer = 'SGD',
     metrics = 'accuracy'
 )

# Fit the model to the data
model2 %>% fit(iris_train,iris_train_Labels, epochs = 500, batch_size = 5, validation_split = 0.2)

# Evaluate the model
score <- model2 %>% evaluate(iris_test, iris_test_Labels, batch_size = 128)

# Print the score
print(score)


```

## 12.3 Optimizing parameters
```{R}
# Initialize the sequential model
model3 <- keras_model_sequential() 

# Add layers to model
model3 %>% 
    layer_dense(units = 8, activation = 'relu', input_shape = c(4)) %>% 
    layer_dense(units = 3, activation = 'softmax')



# Compile the model
model3 %>% compile(
     loss = 'categorical_crossentropy',
     optimizer = 'adam',
     metrics = 'accuracy'
 )

# Fit the model to the data
model3 %>% fit(iris_train,iris_train_Labels, epochs = 500, batch_size = 5, validation_split = 0.2)

# Evaluate the model
score <- model3 %>% evaluate(iris_test, iris_test_Labels, batch_size = 128)

# Print the score
print(score)

```

# 13.save the model
```{R}
save_model_hdf5(model,"iris_model.h5")
mode <- load_model_hdf5("iris_model.h5")

json_string <- model_to_json(model)
model <- model_from_json(json_string)
```