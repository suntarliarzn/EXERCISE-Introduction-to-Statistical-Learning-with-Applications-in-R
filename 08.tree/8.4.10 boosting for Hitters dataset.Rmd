---
title: "8.4.10 boosting for Hitters dataset"
output: github_document
---

# 1. data preparation

```{R}
library(ISLR)
library(gbm)
attach(Hitters)
head(Hitters)
str(Hitters)
summary(Hitters)
dim(Hitters)
hitters=na.omit(Hitters)
dim(hitters)
hitters$Salary=log(hitters$Salary)
```

# 2.training
```{R}
set.seed(101)
hitter.train=hitters[1:200,]
hitter.test=hitters[-c(1:200),]
salary.test=hitter.test$Salary
```


# 3.boosting for a wide range of lamda
```{R}
set.seed(101)
lamb = seq(from=0.001, to=0.1, by=0.001)
mse.hitter=rep(NA,100)
for (n in 1:100){
  boost.hitter.lambda=gbm(Salary~., data=hitter.train, distribution = "gaussian", n.trees = 1000, shrinkage = lamb[n])
  pred.hitter.lambda=predict(boost.hitter.lambda, newdata=hitter.test, n.trees = 1000)
  mse.hitter[n]=mean((pred.hitter.lambda-salary.test)^2)
}
plot(lamb, mse.hitter, pch=19, ylab="MSE", xlab = "lambda", type="b")
lamb[which.min(mse.hitter)]
# lambda=0.085  min.mse.test=0.2448
```

# 4.multiple linear regression
```{R}
lm.hitter=lm(Salary~.,hitter.train)
pred.hitter=predict(lm.hitter, hitter.test)
mse.lm=mean((pred.hitter-salary.test)^2)
mse.lm
# mse.lm=0.4918
```

# 5.lasso
```{R}
library(glmnet)
x=model.matrix(Salary~.,hitters)[,-1]
y=hitters$Salary
lasso.mod=glmnet(x[1:200,],y[1:200], alpha = 1)
lasso.hitter=cv.glmnet(x[1:200,], y[1:200])
bestlamda=lasso.hitter$lambda.min
lasso.pred=predict(lasso.mod, s=bestlamda,newx=x[-c(1:200),])
mse.lasso=mean((lasso.pred-salary.test)^2)
# mse.lasso= 0.4706
```

# 6.best boosting model
```{R}
boost.best=gbm(Salary~., data=hitter.train, distribution = "gaussian", n.trees = 1000, shrinkage = 0.085)
summary(boost.best)

```

# 7.bagging
```{R}
library(randomForest)
rf.hitter=randomForest(Salary~.,data=hitter.train, mtry=19)
rf.hitter
summary(rf.hitter)
pred.rf=predict(rf.hitter,hitter.test)
mse.rf=mean((pred.rf-salary.test)^2)
# mse.rf=0.2316679
```
