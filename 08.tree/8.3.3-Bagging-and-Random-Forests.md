8.3.3 Bagging and Random Forests
================

# 1. Preparation of the data

``` r
library(tree)
```

    ## Warning: package 'tree' was built under R version 3.6.3

``` r
library(MASS)
```

    ## Warning: package 'MASS' was built under R version 3.6.3

``` r
attach(Boston)
set.seed(2)
dim(Boston)
```

    ## [1] 506  14

``` r
train=sample(1:nrow(Boston),300)
boston.train=Boston[train,]
boston.test=Boston[-train,]
medv.test=medv[-train]
```

# 2.Fit the Random Forest with training set

``` r
# install.packages("randomForest")
library(randomForest)
```

    ## Warning: package 'randomForest' was built under R version 3.6.3

    ## randomForest 4.6-14

    ## Type rfNews() to see new features/changes/bug fixes.

``` r
rd.boston=randomForest(medv~.,data=Boston, subset=train)
rd.boston
```

    ## 
    ## Call:
    ##  randomForest(formula = medv ~ ., data = Boston, subset = train) 
    ##                Type of random forest: regression
    ##                      Number of trees: 500
    ## No. of variables tried at each split: 4
    ## 
    ##           Mean of squared residuals: 12.65663
    ##                     % Var explained: 84.7

``` r
summary(rd.boston)
```

    ##                 Length Class  Mode     
    ## call              4    -none- call     
    ## type              1    -none- character
    ## predicted       300    -none- numeric  
    ## mse             500    -none- numeric  
    ## rsq             500    -none- numeric  
    ## oob.times       300    -none- numeric  
    ## importance       13    -none- numeric  
    ## importanceSD      0    -none- NULL     
    ## localImportance   0    -none- NULL     
    ## proximity         0    -none- NULL     
    ## ntree             1    -none- numeric  
    ## mtry              1    -none- numeric  
    ## forest           11    -none- list     
    ## coefs             0    -none- NULL     
    ## y               300    -none- numeric  
    ## test              0    -none- NULL     
    ## inbag             0    -none- NULL     
    ## terms             3    terms  call

``` r
#msr are based on out of bag estimates
plot(rd.boston)
```

![](8.3.3-Bagging-and-Random-Forests_files/figure-gfm/unnamed-chunk-2-1.png)<!-- -->

# 3.select the best mtry by the training dataset

``` r
oob.err=rep(NA,13)
test.err=rep(NA,13)
for (mtry in 1:13){
  fit=randomForest(medv~., data=Boston, subset=train, mtry=mtry, mtree=400)
  oob.err[mtry]=fit$mse[400]
  pred=predict(fit,boston.test)
  test.err[mtry]=mean((pred-medv.test)^2)
}
matplot(1:mtry,cbind(oob.err, test.err),pch=19,col=c("red","blue"),type="b", ylab="MSE")
legend("topright", legend=c("OOB","Test"),pch=19, col=c("red","blue"))
```

![](8.3.3-Bagging-and-Random-Forests_files/figure-gfm/unnamed-chunk-3-1.png)<!-- -->

``` r
# mtry=5
```

# 4. importance of the variables

``` r
rd.boston.best=randomForest(medv~.,data=Boston, subset=train, mtry=5, importance =TRUE)
importance (rd.boston.best)
```

    ##           %IncMSE IncNodePurity
    ## crim    15.026730    1416.98466
    ## zn       4.020740     173.89944
    ## indus    9.670857    1239.85051
    ## chas     1.911432      72.72223
    ## nox     14.847464    1658.68743
    ## rm      31.764137    6762.65069
    ## age     12.289446     755.20994
    ## dis     15.596035    1600.60054
    ## rad      6.166466     188.22151
    ## tax      9.672473     525.39894
    ## ptratio 13.390065    1343.65532
    ## black    7.417074     450.73433
    ## lstat   32.071264    7996.08275

``` r
varImpPlot (rd.boston.best)
```

![](8.3.3-Bagging-and-Random-Forests_files/figure-gfm/unnamed-chunk-4-1.png)<!-- -->
