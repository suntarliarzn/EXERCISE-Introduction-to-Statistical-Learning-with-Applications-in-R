8.4.9 Fitting regression tree for OJ dataset
================

# 1. data preparation

``` r
set.seed(101)
library(ISLR)
```

    ## Warning: package 'ISLR' was built under R version 3.6.3

``` r
library(tree)
```

    ## Warning: package 'tree' was built under R version 3.6.3

``` r
library(randomForest)
```

    ## Warning: package 'randomForest' was built under R version 3.6.3

    ## randomForest 4.6-14

    ## Type rfNews() to see new features/changes/bug fixes.

``` r
attach(OJ)
?OJ
```

    ## starting httpd help server ...

    ##  done

``` r
head(OJ)
```

    ##   Purchase WeekofPurchase StoreID PriceCH PriceMM DiscCH DiscMM SpecialCH
    ## 1       CH            237       1    1.75    1.99   0.00    0.0         0
    ## 2       CH            239       1    1.75    1.99   0.00    0.3         0
    ## 3       CH            245       1    1.86    2.09   0.17    0.0         0
    ## 4       MM            227       1    1.69    1.69   0.00    0.0         0
    ## 5       CH            228       7    1.69    1.69   0.00    0.0         0
    ## 6       CH            230       7    1.69    1.99   0.00    0.0         0
    ##   SpecialMM  LoyalCH SalePriceMM SalePriceCH PriceDiff Store7 PctDiscMM
    ## 1         0 0.500000        1.99        1.75      0.24     No  0.000000
    ## 2         1 0.600000        1.69        1.75     -0.06     No  0.150754
    ## 3         0 0.680000        2.09        1.69      0.40     No  0.000000
    ## 4         0 0.400000        1.69        1.69      0.00     No  0.000000
    ## 5         0 0.956535        1.69        1.69      0.00    Yes  0.000000
    ## 6         1 0.965228        1.99        1.69      0.30    Yes  0.000000
    ##   PctDiscCH ListPriceDiff STORE
    ## 1  0.000000          0.24     1
    ## 2  0.000000          0.24     1
    ## 3  0.091398          0.23     1
    ## 4  0.000000          0.00     1
    ## 5  0.000000          0.00     0
    ## 6  0.000000          0.30     0

``` r
str(OJ)
```

    ## 'data.frame':    1070 obs. of  18 variables:
    ##  $ Purchase      : Factor w/ 2 levels "CH","MM": 1 1 1 2 1 1 1 1 1 1 ...
    ##  $ WeekofPurchase: num  237 239 245 227 228 230 232 234 235 238 ...
    ##  $ StoreID       : num  1 1 1 1 7 7 7 7 7 7 ...
    ##  $ PriceCH       : num  1.75 1.75 1.86 1.69 1.69 1.69 1.69 1.75 1.75 1.75 ...
    ##  $ PriceMM       : num  1.99 1.99 2.09 1.69 1.69 1.99 1.99 1.99 1.99 1.99 ...
    ##  $ DiscCH        : num  0 0 0.17 0 0 0 0 0 0 0 ...
    ##  $ DiscMM        : num  0 0.3 0 0 0 0 0.4 0.4 0.4 0.4 ...
    ##  $ SpecialCH     : num  0 0 0 0 0 0 1 1 0 0 ...
    ##  $ SpecialMM     : num  0 1 0 0 0 1 1 0 0 0 ...
    ##  $ LoyalCH       : num  0.5 0.6 0.68 0.4 0.957 ...
    ##  $ SalePriceMM   : num  1.99 1.69 2.09 1.69 1.69 1.99 1.59 1.59 1.59 1.59 ...
    ##  $ SalePriceCH   : num  1.75 1.75 1.69 1.69 1.69 1.69 1.69 1.75 1.75 1.75 ...
    ##  $ PriceDiff     : num  0.24 -0.06 0.4 0 0 0.3 -0.1 -0.16 -0.16 -0.16 ...
    ##  $ Store7        : Factor w/ 2 levels "No","Yes": 1 1 1 1 2 2 2 2 2 2 ...
    ##  $ PctDiscMM     : num  0 0.151 0 0 0 ...
    ##  $ PctDiscCH     : num  0 0 0.0914 0 0 ...
    ##  $ ListPriceDiff : num  0.24 0.24 0.23 0 0 0.3 0.3 0.24 0.24 0.24 ...
    ##  $ STORE         : num  1 1 1 1 0 0 0 0 0 0 ...

``` r
df=sample(1:nrow(OJ),800)
oj.train=OJ[df,]
oj.test=OJ[-df,]
purchase.test=Purchase[-df]
```

# 2. fitting the tree with purchase

``` r
oj.tree=tree(Purchase~.,oj.train)
summary(oj.tree)
```

    ## 
    ## Classification tree:
    ## tree(formula = Purchase ~ ., data = oj.train)
    ## Variables actually used in tree construction:
    ## [1] "LoyalCH"       "PriceDiff"     "ListPriceDiff"
    ## Number of terminal nodes:  8 
    ## Residual mean deviance:  0.7634 = 604.6 / 792 
    ## Misclassification error rate: 0.1662 = 133 / 800

``` r
plot(oj.tree)
text(oj.tree, cex=0.8)
```

![](8.4.9-Fitting-regression-tree-for-OJ-dataset_files/figure-gfm/unnamed-chunk-2-1.png)<!-- -->

``` r
# training error rate=0.1525
```

# 3.predict on the test data

``` r
oj.pred=predict(oj.tree, oj.test,type="class")
oj.tree.mse=table(oj.pred,purchase.test)
#        purchase.test
# oj.pred  CH  MM
#      CH 151  35
#      MM  14  70
# (35+14)/(151+35+14+70)= 0.1814815
```

# 4.cv tree

``` r
set.seed(2)
cv.oj =cv.tree(oj.tree,FUN=prune.misclass)
cv.oj
```

    ## $size
    ## [1] 8 5 2 1
    ## 
    ## $dev
    ## [1] 164 165 163 297
    ## 
    ## $k
    ## [1] -Inf    0    7  143
    ## 
    ## $method
    ## [1] "misclass"
    ## 
    ## attr(,"class")
    ## [1] "prune"         "tree.sequence"

``` r
plot(cv.oj$size ,cv.oj$dev ,type="b")
```

![](8.4.9-Fitting-regression-tree-for-OJ-dataset_files/figure-gfm/unnamed-chunk-4-1.png)<!-- -->

``` r
# best size =2
```

# 5. prune the tree with best tree size

``` r
prune.oj=prune.misclass(oj.tree, best = 2)
plot(prune.oj)
text(prune.oj)
```

![](8.4.9-Fitting-regression-tree-for-OJ-dataset_files/figure-gfm/unnamed-chunk-5-1.png)<!-- -->

``` r
summary(prune.oj)
```

    ## 
    ## Classification tree:
    ## snip.tree(tree = oj.tree, nodes = 2:3)
    ## Variables actually used in tree construction:
    ## [1] "LoyalCH"
    ## Number of terminal nodes:  2 
    ## Residual mean deviance:  0.9675 = 772 / 798 
    ## Misclassification error rate: 0.1925 = 154 / 800

``` r
# training error rate=0.195
```

# 6.predict on the test data with pruned tree

``` r
prune.oj.pred=predict(prune.oj, oj.test,type="class")
prune.oj.mse=table(prune.oj.pred,purchase.test)
#              purchase.test
# prune.oj.pred  CH  MM
#            CH 141  24
#            MM  24  81
# (24+24)/(141+24+24+81)=0.1777778
```
