9.6.1 Support Vector Classifier
================

# 1.data preparation

``` r
set.seed(10111)
x = matrix(rnorm(40), 20, 2)
y = rep(c(-1,1), c(10,10))
x[y == 1,] = x[y == 1,] + 1
plot(x, col=y+3, pch=19)
```

![](9.6.1-Support-Vector-Classifier_files/figure-gfm/unnamed-chunk-1-1.png)<!-- -->

# 2. svm function

``` r
# install.packages("e1071")
library(e1071)
```

    ## Warning: package 'e1071' was built under R version 3.6.3

``` r
dat=data.frame(x, y=as.factor(y))
svm.fit=svm(y~., data=dat, kernel="linear", cost=20, scale=F)
summary(svm.fit)
```

    ## 
    ## Call:
    ## svm(formula = y ~ ., data = dat, kernel = "linear", cost = 20, scale = F)
    ## 
    ## 
    ## Parameters:
    ##    SVM-Type:  C-classification 
    ##  SVM-Kernel:  linear 
    ##        cost:  20 
    ## 
    ## Number of Support Vectors:  6
    ## 
    ##  ( 3 3 )
    ## 
    ## 
    ## Number of Classes:  2 
    ## 
    ## Levels: 
    ##  -1 1

``` r
plot(svm.fit, dat)
```

![](9.6.1-Support-Vector-Classifier_files/figure-gfm/unnamed-chunk-2-1.png)<!-- -->

``` r
svm.fit$index
```

    ## [1]  1  4 10 14 16 20

# 3.make a better plot

``` r
make.grid = function(x, n = 75) {
  grange = apply(x, 2, range) #range of each column of x1 and x2
  x1 = seq(from = grange[1,1], to = grange[2,1], length = n)
  x2 = seq(from = grange[1,2], to = grange[2,2], length = n)
  expand.grid(X1 = x1, X2 = x2)
}
xgrid= make.grid(x)
ygrid= predict(svm.fit, xgrid)
plot(xgrid, col = c("red","blue")[as.numeric(ygrid)], pch = 20, cex = .2)
points(x, col = y + 3, pch = 19)
points(x[svm.fit$index,], pch = 5, cex = 2)
```

![](9.6.1-Support-Vector-Classifier_files/figure-gfm/unnamed-chunk-3-1.png)<!-- -->

# 4.draw the margin

``` r
beta = drop(t(svm.fit$coefs)%*%x[svm.fit$index,]) # drop converts a table into an array
beta0 = svm.fit$rho
plot(xgrid, col = c("red","blue")[as.numeric(ygrid)], pch = 20, cex = .2)
points(x, col = y + 3, pch = 19)
points(x[svm.fit$index,], pch = 5, cex = 2)
abline(beta0 / beta[2], -beta[1] / beta[2])
abline((beta0 - 1) / beta[2], -beta[1] / beta[2], lty = 2)
abline((beta0 + 1) / beta[2], -beta[1] / beta[2], lty = 2)
```

![](9.6.1-Support-Vector-Classifier_files/figure-gfm/unnamed-chunk-4-1.png)<!-- -->

# 5.cross-validation

``` r
set.seed(1)
tune.out=tune(svm, y~., data=dat, kernel ="linear",ranges=list(cost=seq(0.001,100,10)))
summary(tune.out)
```

    ## 
    ## Parameter tuning of 'svm':
    ## 
    ## - sampling method: 10-fold cross validation 
    ## 
    ## - best parameters:
    ##    cost
    ##  20.001
    ## 
    ## - best performance: 0.1 
    ## 
    ## - Detailed performance results:
    ##      cost error dispersion
    ## 1   0.001  0.55  0.4377975
    ## 2  10.001  0.15  0.2415229
    ## 3  20.001  0.10  0.2108185
    ## 4  30.001  0.10  0.2108185
    ## 5  40.001  0.10  0.2108185
    ## 6  50.001  0.10  0.2108185
    ## 7  60.001  0.10  0.2108185
    ## 8  70.001  0.10  0.2108185
    ## 9  80.001  0.10  0.2108185
    ## 10 90.001  0.10  0.2108185

``` r
bestmod=tune.out$best.model
```

\#6. prediction on test dataset

``` r
set.seed(1)
xtest=matrix(rnorm (20*2), ncol=2)
ytest=sample(c(-1,1) , 20, rep=TRUE)
xtest[ytest==1,]=xtest[ytest ==1,] + 1
testdat=data.frame(X1=xtest[,1],X2=xtest[,2], y=as.factor(ytest))
ypred=predict(bestmod, testdat)
table(predict=ypred,truth=testdat$y)
```

    ##        truth
    ## predict -1 1
    ##      -1  8 2
    ##      1   3 7
