Untitled
================

`{r setup, include=FALSE} knitr::opts_chunk$set(echo = TRUE)`

8.  In this exercise, we will generate simulated data, and will then use
    this data to perform best subset selection.

<!-- -->

1.  Use the rnorm() function to generate a predictor X of length n =
    100, as well as a noise vector  of length n = 100.
2.  Generate a response vector Y of length n = 100 according to the
    model Y = β0 + β1X + β2X2 + β3X3 + , where β0, β1, β2,and β3 are
    constants of your choice.

``` {r}
set.seed(1)
x=rnorm(100)
noise=rnorm(100)
b0 = 50
b1 = 6
b2 = 3
b3 = 1.5
y=b0+b1*x+b2*x^2+b3*x^3+noise
```

3.  Use the regsubsets() function to perform best subset selection in
    order to choose the best model containing the predictors X,X2,…,X10.
    What is the best model obtained according to Cp,BIC,and adjusted R2?
    Show some plots to provide evidence for your answer, and report the
    coefficients of the best model obtained. Note you will need to use
    the data.frame() function to create a single data set containing
    both X and Y.

``` {r}
library(leaps)
smdata=data.frame(x,x^2,x^3,x^4,x^5,x^6,x^7,x^8,x^9,x^10,y)
regfit.full=regsubsets(y~., data=smdata, nvmax=10)
reg.summary=summary(regfit.full)
reg.summary$cp
reg.summary$bic #best model
reg.summary$adjr2
coef(regfit.full,3)

#plot the result
plot(reg.summary$cp, xlab = "number of variables", ylab = "Cp", type ="l")
points(which.min(reg.summary$cp), reg.summary$cp[which.min(reg.summary$cp)], col = "red", pch = 20)

plot(reg.summary$bic, xlab = "number of variables", ylab = "bic", type ="l")
points(which.min(reg.summary$bic), reg.summary$bic[which.min(reg.summary$bic)], col = "red", pch = 20)

plot(reg.summary$adjr2, xlab = "number of variables", ylab = "adjr2", type ="l")
points(which.max(reg.summary$adjr2), reg.summary$adjr2[which.max(reg.summary$adjr2)], col = "red", pch = 20)
```

4.  Repeat (c), using forward stepwise selection and also using
    backwards stepwise selection. How does your answer compare to the
    results in (c)?

``` {r}
smdata=data.frame(x,x^2,x^3,x^4,x^5,x^6,x^7,x^8,x^9,x^10,y)
regfit.full=regsubsets(y~., data=smdata, nvmax=10, method = "forward")
reg.summary=summary(regfit.full)
reg.summary$cp
reg.summary$bic #best model
reg.summary$adjr2
coef(regfit.full,3)

#plot the result
plot(reg.summary$cp, xlab = "number of variables", ylab = "Cp", type ="l")
points(which.min(reg.summary$cp), reg.summary$cp[which.min(reg.summary$cp)], col = "red", pch = 20)

plot(reg.summary$bic, xlab = "number of variables", ylab = "bic", type ="l")
points(which.min(reg.summary$bic), reg.summary$bic[which.min(reg.summary$bic)], col = "red", pch = 20)

plot(reg.summary$adjr2, xlab = "number of variables", ylab = "adjr2", type ="l")
points(which.max(reg.summary$adjr2), reg.summary$adjr2[which.max(reg.summary$adjr2)], col = "red", pch = 20)

#backward
smdata=data.frame(x,x^2,x^3,x^4,x^5,x^6,x^7,x^8,x^9,x^10,y)
regfit.full=regsubsets(y~., data=smdata, nvmax=10, method = "backward")
reg.summary=summary(regfit.full)
reg.summary$cp
reg.summary$bic #best model
reg.summary$adjr2
coef(regfit.full,3)

#plot the result
plot(reg.summary$cp, xlab = "number of variables", ylab = "Cp", type ="l")
points(which.min(reg.summary$cp), reg.summary$cp[which.min(reg.summary$cp)], col = "red", pch = 20)

plot(reg.summary$bic, xlab = "number of variables", ylab = "bic", type ="l")
points(which.min(reg.summary$bic), reg.summary$bic[which.min(reg.summary$bic)], col = "red", pch = 20)

plot(reg.summary$adjr2, xlab = "number of variables", ylab = "adjr2", type ="l")
points(which.max(reg.summary$adjr2), reg.summary$adjr2[which.max(reg.summary$adjr2)], col = "red", pch = 20)
```

5.  Now fit a lasso model to the simulated data, again using X,X2, …,X10
    as predictors. Use cross-validation to select the optimal value of
    λ. Create plots of the cross-validation error as a function of λ.
    Report the resulting coefficient estimates, and discuss the results
    obtained.

``` {r}
library(glmnet)
x=model.matrix(y~.,smdata)[,-1]
y=smdata$y
cv.lasso=cv.glmnet(x,y, alpha=1)
plot(cv.lasso)
coef(cv.lasso) #best model= 6
bestlamda=cv.lasso$lambda.min #bestlamda=0.045
```

6.  Now generate a response vector Y according to the model Y = β0 +
    β7X7 + , and perform best subset selection and the lasso. Discuss
    the results obtained.

``` {r}
set.seed(1)
b7=5
x=rnorm(100)
noise=rnorm(100)
y2=b0+b7*x^7+noise

#bestset selection
smdata=data.frame(x,x^2,x^3,x^4,x^5,x^6,x^7,x^8,x^9,x^10,y2)
regfit.full=regsubsets(y2~., data=smdata, nvmax=10)
reg.summary=summary(regfit.full)
reg.summary
reg.summary$cp
reg.summary$bic #best model
reg.summary$adjr2
#plot the result
plot(reg.summary$cp, xlab = "number of variables", ylab = "Cp", type ="l")
points(which.min(reg.summary$cp), reg.summary$cp[which.min(reg.summary$cp)], col = "red", pch = 20)

plot(reg.summary$bic, xlab = "number of variables", ylab = "bic", type ="l")
points(which.min(reg.summary$bic), reg.summary$bic[which.min(reg.summary$bic)], col = "red", pch = 20)

plot(reg.summary$adjr2, xlab = "number of variables", ylab = "adjr2", type ="l")
points(which.max(reg.summary$adjr2), reg.summary$adjr2[which.max(reg.summary$adjr2)], col = "red", pch = 20)
coef(regfit.full,1)
```

``` {r}
#lasso
x=model.matrix(y2~.,smdata)[,-1]
y=smdata$y2
lasso.mod=cv.glmnet(x,y, alpha = 1)
plot(lasso.mod, xvar = "lambda", label = TRUE)
coef(lasso.mod)
```
